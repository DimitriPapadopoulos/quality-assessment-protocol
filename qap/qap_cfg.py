
default_cfg = {
    'template_head_for_anat': '${FSLDIR}/data/standard/MNI152_T1_2mm.nii.gz',
    'exclude_zeros': 'False',
    'start_idx': '0',
    'stop_idx': 'End',
    'write_report': 'False',
    'output_directory': '',
    's3_output_creds_path': '',
    'working_directory': '',
    'save_working_dir': 'False',
    'bundle_size': 1,
    'num_processors': 1,
    'memory': 4,
    'run_on_cluster': 'False',
    'cluster_system': 'SGE',
    'parallel_env': 'mpi_smp',
    'write_graph': 'False'
}

config_output_str = """
    Pipeline Configuration
    ----------------------

    Working directory:      {working_directory}
    Save working directory: {save_working_dir}
    Output directory:       {output_directory}
    S3 output credentials:  {s3_output_creds_path}
    ---
    MNI template:           {template_head_for_anat}
    Exclude zeros:          {exclude_zeros}
    Start index:            {start_idx}
    Stop index:             {stop_idx}
    Write report:           {write_report}
    ---
    Bundle size:            {bundle_size}
    Number of processors:   {num_processors}
    Memory (GB):            {memory}
    ---
    Run on cluster:         {run_on_cluster}
    Cluster system:         {cluster_system}
    Parallel environment:   {parallel_env}
    ---
    Write execution graph:  {write_graph}
"""


def write_config(config_file_name, config_dict):
    """ Write out a config file with the parameters in config_dict.

        This would be a lot easier to just use yaml.dump, but we want the
        configuration file to be human readable and to include comments. The
        latter is not currently supported by pyYaml. Maybe in the future?

    :type config_file_name: str
    :param config_file_name: Filename, including path, of the file to be written
    :type confid_dict: dict
    :param config_dict: configuration dictionary (see config_dict defined above)
                        that contains the parameters to be written to the config file
    :return: nothing
    """

    config_file_str = '''
# Preprocessed Connectomes Project Quality Assessment Protocol (QAP)
# Configuration file
#
#
# QAP Parameters
# --------------
#
# Path to a whole head template in MNI space. This is only required if you
# are processing sMRI data. We prefer to use the MNI2mm template distributed
# with FSL, but you are free to use any that you choose.
template_head_for_anat: {template_head_for_anat}

# Many of the QAP measures use background voxels to calculate surrogate
# measures of noise. If some of those voxels have been arbitrarily set to
# zero, by defacing for anonymization or other reasons, it will bias these
# measures. These voxels can be ignored by setting the following flag to
# `True'
exclude_zeros: {exclude_zeros}

# The first few volumes of an fMRI scan may have different signal properties
# due to T1 equilibrium effects. Set this parameter to discard volumes from
# the beginning of the scan.
start_idx: {start_idx}

# Similarly it may be desirable to exclude volumes at the end of the scan
# from the calculation. Set this parameter to the index of the last volume
# to include in calculations, or 'End' for the last volume.
stop_idx: {stop_idx}

# Set the write_report parameter to True if you would like QAP to produce
# PDF reports visualizing the results of the QAP metrics.
write_report: False


# Output Paths:
# -------------
#
# Directory for outputs generated by QAP. This includes a single JSON
# for each sMRI scan and a JSON for each fMRI scan. Outputs can be
# written directly to the AWS S3 cloud service by prepending the output
# path with "s3://bucket_name/".
output_directory: {output_directory}

# AWS credentials will probably be needed if writing to S3, if so
# edit the path to point to the credential file downloaded from AWS.
s3_output_creds_path: {s3_output_creds_path}

# Calculating the QAP measures requires  a variety of intermediary files
# that are derived from the input data (e.g., brain mask, white matter mask).
# These files are written to a working directory that can be automatically
# deleted when QAP completes. Since a lot of files will be written and read
# from this directory, it should ideally be locally connected to the workstation
# (i.e. not a network share) - this is especially important for cluster and
# cloud based calculation.
working_directory: {working_directory}

# If you do not want to automatically delete the working directory at the
# end of QAP, change the following flag to `True'. This is primarily useful
# for debugging.
save_working_dir: {save_working_dir}

# Multicore Parallelization and Bundles
# -------------------------------
#
# The different procedures that are required to calculate QAP measures from
# a dataset (sMRI or fMRI) is represented as a node in the QAP pipeline. Each
# dataset has a separate pipeline that can be executed in parallel using multi-core
# and cluster -based parallelization to achieve high throughput.

# For multicore parallelization, QAP relies on Nipype to execute different steps of
# the pipeline in parallel on the processors available on a single workstation. The
# number of nodes that can be scheduled to execute in parallel is limited by the total
# number of processors and amount of RAM available on the system. Practically the
# amount of parallelization that can be achieved is limited by the dependencies pipeline
# steps. For example, several nodes of the pipeline can be blocked until a node produces
# an intermediary file that they all need. QAP mitigates this problem by combining
# pipelines into a 'bundle' so that pipeline steps for different datasets can execute
# in parallel. We bundle datasets by session to simplify balancing the number of sMRI
# and fMRI data but into each bundle. So the bundle size is in units of sessions. In
# other words, if in your dataset you have two sessions per participant and each session
# contains two fMRI and one sMRI scans, and you specify a bundle size of 10, each bundle
# will receive 10 sMRI and 20 fMRI scans.
#
# When no other resource constrains execution, it may work best to put all of the data
# into a single bundle and allow them to all compete for unused resources. Since intermediary
# files must be retained until a pipeline completes, the space required to store these
# files limits the number of pipelines that can be run in parallel. The working directory
# for structural and functional data are estimated to require about twice the input data
# size in storage. The max bundle size you should use can be estimated from:
#    (size of working dir) / ( twice size of bundle )
# In the previous example the bundle size would be the size of one sMRI plus the two fMRI
# scans.
#
# The parameter below specifies the bundle size to use.
bundle_size: {bundle_size}

# The following parameters specify the number of processors and amount of memory (in GB) that are
# available for multicore processing. If using cluster computing, these are the resources
# available on each cluster node.
num_processors: {num_processors}
memory: {memory}

# Cluster parallelization
# -----------------------
#
# QAP can also make use of cluster computing to increase throughput by running bundles on
# separate nodes of a cluster. QAP will still perform multicore parallelization on each
# cluster node, meaning that the multicore configuration must still be specified if using
# cluster computing. QAP can work with SGE, PBS, or SLURM management system and automatically
# generates all of the cluster configuration files and submits them to the job queue.
#
# To turn on cluster processing, set 'run_on_cluster' to True
run_on_cluster: {run_on_cluster}

# Specify the clustering system ('SGE','PBS','SLURM')
cluster_system: {cluster_system}

# SGE uses 'parallel' environments to determine how jobs should be mapped to nodes.
# Since QAP will be running in multicore, it will request several processors from the
# cluster managements system. Some parallel environments will allow a single job (QAP bundle)
# to be mapped to processors on several different nodes, which is not suitable. QAP requires
# that all of the processors allocated for a bundle be on the same cluster node. This is
# accomplished by setting the 'allocation_rule' of the pe environment to '$pe_slots'.
#
# Specify the name of the parallel environment to use on your cluster. See the QAP web page
# for more information.
parallel_env: {parallel_env}

# Debugging
# -------------------------------

# Produce a graph for visualizing the workflow
write_graph: {write_graph}
    '''

    with open(config_file_name,'w') as ofd:
        ofd.write( config_file_str.format(**config_dict) )

    return 1



if __name__ == "__main__":

    write_config("test_config.yml", default_cfg)

    print(config_output_str.format(**default_cfg))

    import yaml
    test_cfg = yaml.load(open("test_config.yml","r"))
    print(config_output_str.format(**test_cfg))

    assert(test_cfg != default_cfg)